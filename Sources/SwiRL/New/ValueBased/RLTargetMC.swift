//
//  RLTargetMC.swift
//  
//
//  Created by Maxim Volgin on 11/10/2020.
//

import Numerics
import Foundation

public struct RLTargetMC<State: RLState, Value: RLValue>: RLTarget {
    
    public let reward: [Value]
    public let discount: Value
    
    public init(reward: [Value], discount: Value) {
        self.reward = reward
        self.discount = discount
    }
    
    public func callAsFunction() -> Value {
//        RLTargetMC.expectedReturn(reward: reward, discount: discount)
        RLTargetMC.expectedReturnRecursive(reward: reward, discount: discount)
    }
    
    /**
        Monte Carlo estimated return for step `t`
     
        - Note: Gâ‚œ = Râ‚œâ‚Šâ‚ + ğ›„Râ‚œâ‚Šâ‚‚ + ğ›„Â²Râ‚œâ‚Šâ‚ƒ + ğ›„Â³Râ‚œâ‚Šâ‚„ + ... + ğ›„â¿Râ‚™
     
        - Parameters:
            - R: reward from step `t+1` onwards: Râ‚œâ‚Šâ‚ + ... + Râ‚™
            - ğ›„: discount
        - Returns: Äœâ‚œ
     */
//    public static func expectedReturn(reward R: [Value], discount ğ›„: Value) -> Value {
//        R
//        .enumerated()
//            .reduce(0.0) { (G, r) -> Value in
//            G + pow(ğ›„, r.offset) * r.element
//        }
//    }
    
    /**
     Monte Carlo estimated return for step `t`
  
     - Remark: recursive version for reference only, performance is suboptimal
  
     - Note: Gâ‚œ = Râ‚œâ‚Šâ‚ + ğ›„Râ‚œâ‚Šâ‚‚ + ğ›„Â²Râ‚œâ‚Šâ‚ƒ + ğ›„Â³Râ‚œâ‚Šâ‚„ + ... + ğ›„â¿Râ‚™
     
     - Parameters:
         - R: reward from step `t+1` onwards: Râ‚œâ‚Šâ‚ + ... + Râ‚™
         - ğ›„: discount
     - Returns: Äœâ‚œ
     */
    public static func expectedReturnRecursive(reward R: [Value], discount ğ›„: Value) -> Value {
        var mutableR = R
        return mutableR.removeFirst() + ğ›„ * RLTargetMC.expectedReturnRecursive(reward: mutableR, discount: ğ›„)
    }
    
}
