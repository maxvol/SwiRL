//
//  RLTargetQLearning.swift
//  SwiRL
//
//  Created by Maxim Volgin on 10/10/2020.
//

import Foundation

public struct RLTargetQLearning<State: RLState, Action: RLAction, Value: RLValue>: RLStateActionValueTarget {
    
    public let reward: Value
    public let discount: Value
    public let value: (State, Action) -> Value
    public let nextState: State
    public let actionSpace: [Action]
    
    public init(reward: Value, discount: Value, value: @escaping (State, Action) -> Value, nextState: State, actionSpace: [Action]) {
        self.reward = reward
        self.discount = discount
        self.value = value
        self.nextState = nextState
        self.actionSpace = actionSpace
    }
    
    public func callAsFunction() -> Value {
        RLTargetQLearning.expectedReturn(reward: reward, discount: discount, value: value, state: nextState, actionSpace: actionSpace)
    }

    /**
        Q-learning estimated return for step `t`
     
        - Note: Râ‚œâ‚Šâ‚ + ğ›„maxâ‚Q(Sâ‚œâ‚Šâ‚, a)
        - Remark: bootstrapping, 1-step
     
        - Parameters:
            - Râ‚œâ‚Šâ‚: reward
            - Sâ‚œâ‚Šâ‚: next state
            - Q: value function
            - ğ›„: discount
        - Returns: Äœâ‚œ
     */
    public static func expectedReturn(reward Râ‚œâ‚Šâ‚: Value, discount ğ›„: Value, value Q: (State, Action) -> Value, state Sâ‚œâ‚Šâ‚: State, actionSpace A: [Action]) -> Value {
        Râ‚œâ‚Šâ‚ + ğ›„*A.map{ a in Q(Sâ‚œâ‚Šâ‚, a) }.max()!
    }
    
}

